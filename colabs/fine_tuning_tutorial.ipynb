{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiBSu3YkEcoX"
   },
   "source": [
    "Copyright 2024 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning the 2B Gemma model with flax\n",
    "\n",
    "In this tutorial you will learn how to fine-tune the 2B Gemma model for a simple translation task. To run this colab, you will need to use a TPU v4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /root/.local/lib/python3.10/site-packages (0.2.9)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from kagglehub) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->kagglehub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->kagglehub) (2024.7.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XpSw-_4EEcoY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# @title Installation\n",
    "!pip install git+https://github.com/google-deepmind/gemma.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLafhtv3Rg5F"
   },
   "source": [
    "## Downloading the checkpoint\n",
    "\n",
    "\"To use Gemma's checkpoints, you'll need a Kaggle account and API key. Here's how to get them:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.11-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Downloading widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8q5seOhcUBhx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78234cc02e743b6bec571e01cd1476b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set your Kaggle credentials as environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = 'felarof99'\n",
    "os.environ['KAGGLE_KEY'] = '1bf21523d3337386eac3aa5d61d62b7c'\n",
    "\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCZSmEVDVv6O"
   },
   "source": [
    "If everything went well, you should see:\n",
    "```\n",
    "Kaggle credentials set.\n",
    "Kaggle credentials successfully validated.\n",
    "```\n",
    "\n",
    "Now select and download the checkpoint you want to try. On a single host, only the 2b model can fit in memory for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PEefz8wEcoY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "VARIANT = '2b-it' # @param ['2b', '2b-it'] {type:\"string\"}\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(weights_dir, VARIANT)\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow)\n",
      "  Downloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow)\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow)\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.2.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow)\n",
      "  Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, markdown, h5py, grpcio, google-pasta, gast, astunparse, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.27.3\n",
      "    Uninstalling protobuf-5.27.3:\n",
      "      Successfully uninstalled protobuf-5.27.3\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.65.4 h5py-3.11.0 keras-3.4.1 libclang-18.1.1 markdown-3.6 namex-0.0.8 optree-0.12.1 protobuf-4.25.4 tensorboard-2.17.0 tensorboard-data-server-0.7.2 tensorflow-2.17.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 werkzeug-3.0.3 wrapt-1.16.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Collecting click (from tensorflow_datasets)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (4.25.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (17.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow_datasets)\n",
      "  Downloading simple_parsing-0.1.5-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml (from tensorflow_datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Collecting array-record>=0.5.0 (from tensorflow_datasets)\n",
      "  Downloading array_record-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
      "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.7.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2024.2.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (6.4.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.7.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser~=0.15 in /usr/local/lib/python3.10/site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Downloading tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading dm_tree-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=f2ee65acdb55e1509b189ac4fbeca25acb764b3b91ed81fd973dca248e73ab7e\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/4e/28/3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, toml, simple-parsing, protobuf, promise, immutabledict, click, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.4\n",
      "    Uninstalling protobuf-4.25.4:\n",
      "      Successfully uninstalled protobuf-4.25.4\n",
      "Successfully installed array-record-0.5.1 click-8.1.7 dm-tree-0.1.8 immutabledict-4.2.0 promise-2.3 protobuf-3.20.3 simple-parsing-0.1.5 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.6 toml-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:15:29.603626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 19:15:29.659854: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 19:15:29.677080: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 19:15:31.517412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# We will use tensorflow to handle the dataset\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejQhgtjbEcoY"
   },
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "### The MTNT dataset\n",
    "\n",
    "In this tutorial, we will use the MTNT dataset, from the paper [MTNT: A Testbed for Machine Translation of Noisy Text](https://arxiv.org/abs/1809.00388). This dataset is directly available in the [TensorFlow dataset catalog](https://www.tensorflow.org/datasets/catalog/mtnt).\n",
    "\n",
    "More precisely we will focus on the English to French translation.\n",
    "\n",
    "But let's have a look at the data themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "pg8SfQH0EcoY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 35.08 MiB (download: 35.08 MiB, generated: 11.33 MiB, total: 46.41 MiB) to /root/tensorflow_datasets/mtnt/en-fr/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   0%|          | 0/35 [00:00<?, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   3%|▎         | 1/35 [00:00<00:13,  2.51 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   6%|▌         | 2/35 [00:00<00:06,  4.80 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:   9%|▊         | 3/35 [00:00<00:04,  6.92 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  11%|█▏        | 4/35 [00:00<00:03,  8.87 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  14%|█▍        | 5/35 [00:00<00:02, 10.67 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  17%|█▋        | 6/35 [00:00<00:02, 12.36 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  20%|██        | 7/35 [00:00<00:02, 13.92 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  23%|██▎       | 8/35 [00:00<00:01, 15.38 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  26%|██▌       | 9/35 [00:00<00:01, 16.75 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  29%|██▊       | 10/35 [00:00<00:01, 18.03 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  31%|███▏      | 11/35 [00:00<00:01, 19.25 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  34%|███▍      | 12/35 [00:00<00:01, 20.39 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  37%|███▋      | 13/35 [00:00<00:01, 21.46 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  40%|████      | 14/35 [00:00<00:00, 22.47 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  43%|████▎     | 15/35 [00:00<00:00, 23.42 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  46%|████▌     | 16/35 [00:00<00:00, 24.32 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  49%|████▊     | 17/35 [00:00<00:00, 25.16 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  51%|█████▏    | 18/35 [00:00<00:00, 25.97 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  54%|█████▍    | 19/35 [00:00<00:00, 26.75 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  57%|█████▋    | 20/35 [00:00<00:00, 27.48 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  60%|██████    | 21/35 [00:00<00:00, 28.20 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  63%|██████▎   | 22/35 [00:00<00:00, 28.88 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  66%|██████▌   | 23/35 [00:00<00:00, 29.54 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  69%|██████▊   | 24/35 [00:00<00:00, 30.16 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  71%|███████▏  | 25/35 [00:00<00:00, 30.66 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  74%|███████▍  | 26/35 [00:00<00:00, 31.23 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  77%|███████▋  | 27/35 [00:00<00:00, 31.78 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  80%|████████  | 28/35 [00:00<00:00, 32.31 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  83%|████████▎ | 29/35 [00:00<00:00, 32.82 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  86%|████████▌ | 30/35 [00:00<00:00, 33.31 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  89%|████████▊ | 31/35 [00:00<00:00, 33.78 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  91%|█████████▏| 32/35 [00:00<00:00, 34.23 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  94%|█████████▍| 33/35 [00:00<00:00, 34.64 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...:  97%|█████████▋| 34/35 [00:00<00:00, 35.06 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 100%|██████████| 35/35 [00:00<00:00, 35.46 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  1.00 url/s]\n",
      "Dl Size...: 100%|██████████| 35/35 [00:00<00:00, 35.22 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.01s/ url]\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 34.81 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.02s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 34.31 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.03s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 33.87 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.04s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 33.57 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.06s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 33.16 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.07s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 32.73 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.08s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 32.42 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.26s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 27.35 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.35s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 25.78 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.40s/ url]]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 24.99 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.58s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 21.82 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 20.83 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.72s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 20.25 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.73s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 20.20 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.74s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 20.09 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.75s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 20.00 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.76s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 19.92 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.77s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 19.83 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.85s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 18.85 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.89s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 18.47 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.96s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 17.84 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.98s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 17.66 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:01<00:00,  1.99s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:01<00:00, 17.59 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.49 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.01s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.41 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.02s/ url]s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.34 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/26 [00:02<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.03s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.27 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.03s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.22 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.04s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.17 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.04s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.14 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.05s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.10 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.05s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.07 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.06s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 17.03 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.06s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.99 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.07s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.96 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.07s/ url]02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.92 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.07s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.89 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.08s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.85 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.08s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.82 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.09s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.79 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.09s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.75 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.10s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.72 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.10s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.68 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.10s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.65 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.11s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.62 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.11s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.58 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.12s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.55 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.12s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.52 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.13s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.48 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.13s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.45 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.13s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.42 MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.14s/ url].02s/ file]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.38 MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 26/26 [00:02<00:00, 12.04 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 100%|██████████| 35/35 [00:02<00:00, 16.17 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.17s/ url]\n",
      "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]\n",
      "Generating train examples...:   0%|          | 0/35692 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating train examples...:  16%|█▌        | 5658/35692 [00:01<00:05, 5657.05 examples/s]\u001b[A\n",
      "Generating train examples...:  32%|███▏      | 11440/35692 [00:02<00:04, 5730.30 examples/s]\u001b[A\n",
      "Generating train examples...:  48%|████▊     | 17171/35692 [00:03<00:03, 5668.55 examples/s]\u001b[A\n",
      "Generating train examples...:  64%|██████▍   | 22841/35692 [00:04<00:02, 5651.02 examples/s]\u001b[A\n",
      "Generating train examples...:  80%|███████▉  | 28505/35692 [00:05<00:01, 5655.33 examples/s]\u001b[A\n",
      "Generating train examples...:  96%|█████████▌| 34238/35692 [00:06<00:00, 5681.52 examples/s]\u001b[A\n",
      "                                                                                            \u001b[A\n",
      "Shuffling /root/tensorflow_datasets/mtnt/en-fr/incomplete.Q3486T_1.0.0/mtnt-train.tfrecord*...:   0%|          | 0/35692 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating splits...:  33%|███▎      | 1/3 [00:06<00:13,  6.61s/ splits]                                                                        \u001b[A\n",
      "Generating test examples...:   0%|          | 0/1020 [00:00<?, ? examples/s]\u001b[A\n",
      "                                                                            \u001b[A\n",
      "Shuffling /root/tensorflow_datasets/mtnt/en-fr/incomplete.Q3486T_1.0.0/mtnt-test.tfrecord*...:   0%|          | 0/1020 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating splits...:  67%|██████▋   | 2/3 [00:06<00:02,  2.85s/ splits]                                                                      \u001b[A\n",
      "Generating valid examples...:   0%|          | 0/811 [00:00<?, ? examples/s]\u001b[A\n",
      "                                                                            \u001b[A\n",
      "Shuffling /root/tensorflow_datasets/mtnt/en-fr/incomplete.Q3486T_1.0.0/mtnt-valid.tfrecord*...:   0%|          | 0/811 [00:00<?, ? examples/s]\u001b[A\n",
      "                                                                                                                                              \u001b[A\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mtnt downloaded and prepared to /root/tensorflow_datasets/mtnt/en-fr/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Example 0:\n",
      "dst: b'Le groupe de \" toutes les \\xc3\\xa9toiles potentielles de la conf\\xc3\\xa9rence de l\\'Est mais qui ne s\\'en sortent pas dans le groupe de l\\'Ouest \".'\n",
      "src: b'The group of \\xe2\\x80\\x9ceastern conference potential all stars but not making it in the West\\xe2\\x80\\x9d group.'\n",
      "\n",
      "Example 1:\n",
      "dst: b\"Kameron est-elle un peu aigrie de son manque de temps \\xc3\\xa0 l'\\xc3\\xa9cran ?\"\n",
      "src: b'Is Kameron a Little Salty About Her Lack of Air Time?'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:15:54.179739: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "ds = tfds.load(\"mtnt/en-fr\", split=\"train\")\n",
    "ds = ds.take(2)\n",
    "ds = ds.as_numpy_iterator()\n",
    "for idx, example in enumerate(ds):\n",
    "  print(f'Example {idx}:')\n",
    "  for key, val in example.items():\n",
    "    print(f'{key}: {val}')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYy4EJDsEcoY"
   },
   "source": [
    "Each sample in the dataset contains two entries:\n",
    "- 'src': The original English sentence.\n",
    "- 'dst': The corresponding French translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYC42hJgEcoY"
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "Let's start by loading our vocabulary base tokenizer, which we'll construct using the [SentencePiece](https://github.com/google/sentencepiece) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/kagglehub/models/google/gemma/Flax/2b-it/2/tokenizer.model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "TpyG5YW1EcoY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.Load(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab2MSf-qEcoY"
   },
   "source": [
    "Let's customize `SentencePieceProcessor` for our English-to-French translation task. Since we're fine-tuning the English-only Gemma 2B model, we need a few adjustments:\n",
    "\n",
    "- **Input Prefix**: Adding a common prefix to each input signals the translation task. For example we could go with a prompt like `Translate this into French: [INPUT_SENTENCE]`.\n",
    "\n",
    "- **Translation Start suffix**: We add a suffix at the end of each prompt tells the model exactly when to begin the translation process. A new line should do the job.\n",
    "\n",
    "- **LM Tokens**: Gemma models expect a *beginning of sequence* token at the beginning of each sequence. Similarly, we need to add an *end of sequence* token at the end of each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "L9cjK0uxEcoY"
   },
   "outputs": [],
   "source": [
    "class GemmaTokenizer:\n",
    "  \"\"\"Custom wrapper around a SentencePieceProcessor for tensorflow.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               spm_processor: spm.SentencePieceProcessor):\n",
    "    self._spm_processor = spm_processor\n",
    "\n",
    "  @property\n",
    "  def pad_id(self) -> int:\n",
    "    \"\"\"Fast access to the pad id.\"\"\"\n",
    "    return self._spm_processor.pad_id()\n",
    "\n",
    "  def tokenize(self,\n",
    "               example: str | bytes,\n",
    "               prefix: str = '',\n",
    "               suffix: str = '',\n",
    "               add_bos: bool = True,\n",
    "               add_eos: bool = True) -> jax.Array:\n",
    "    \"\"\"\n",
    "    Tokenization function.\n",
    "\n",
    "    Args:\n",
    "      example: input string to tokenize.\n",
    "      prefix:  prefix to add to the input string.\n",
    "      suffix:  suffix to add to the input string.\n",
    "      add_bos: if True, add a beginning of sequence token at the start of the\n",
    "               tokenized sequence.\n",
    "      add_eos: if True, add an end of sequence token at the end of the tokenized\n",
    "               sequence.\n",
    "    Returns:\n",
    "      Tokens corresponding to the input string.\n",
    "    \"\"\"\n",
    "    int_list = []\n",
    "    if add_bos:\n",
    "      int_list.append(self._spm_processor.bos_id())\n",
    "    int_list.extend(self._spm_processor.EncodeAsIds(prefix + example + suffix))\n",
    "    if add_eos:\n",
    "      int_list.append(self._spm_processor.eos_id())\n",
    "\n",
    "    return jnp.array(int_list, dtype=jnp.int32)\n",
    "\n",
    "  def tokenize_tf_op(self,\n",
    "                     str_tensor: tf.Tensor,\n",
    "                     prefix: str = '',\n",
    "                     suffix: str = '',\n",
    "                     add_bos: bool = True,\n",
    "                     add_eos: bool = True) -> tf.Tensor:\n",
    "    \"\"\"Tensforflow operator for the tokenize function.\"\"\"\n",
    "    encoded = tf.numpy_function(\n",
    "        self.tokenize,\n",
    "        [str_tensor, prefix, suffix, add_bos, add_eos],\n",
    "        tf.int32)\n",
    "    encoded.set_shape([None])\n",
    "    return encoded\n",
    "\n",
    "  def to_string(self, tokens: jax.Array) -> str:\n",
    "    \"\"\"Convert an array of tokens to a string.\"\"\"\n",
    "    return self._spm_processor.EncodeIds(tokens.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xuCVkurEcoY"
   },
   "source": [
    "Now let's try our custom tokenizer on the MTNT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "xEA-97ioEcoY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "src: [     2  49688    736   1280   6987 235292    108    651   2778    576\n",
      "   1080 104745  11982   5736    832   8995    901    780   3547    665\n",
      "    575    573   4589 235369   2778 235265    108]\n",
      "dst: [  2025  29653    581    664  16298   1437  55563  41435   7840    581\n",
      "    683 111452    581    533 235303   9776   4108   2459    679    485\n",
      " 235303    479   6728    579   1806   2499    709  29653    581    533\n",
      " 235303 101323  16054      1]\n",
      "\n",
      "Example 1:\n",
      "src: [     2  49688    736   1280   6987 235292    108   2437  87150    477\n",
      "    476  11709 230461   8045   3636  40268    576   4252   4897 235336\n",
      "    108]\n",
      "dst: [213606    477   1455 235290   3510    748   8268 191017   2809    581\n",
      "   2032  69972    581  11495   1305    533 235303  65978   1654      1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:16:16.130451: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GemmaTokenizer(vocab)\n",
    "\n",
    "def tokenize_source(tokenizer, example: tf.Tensor):\n",
    "  return tokenizer.tokenize_tf_op(example,\n",
    "                                  prefix='Translate this into French:\\n',\n",
    "                                  suffix='\\n',\n",
    "                                  add_bos=True,\n",
    "                                  add_eos=False)\n",
    "def tokenize_destination(tokenizer, example: tf.Tensor):\n",
    "  return tokenizer.tokenize_tf_op(example,\n",
    "                                  add_bos=False,\n",
    "                                  add_eos=True)\n",
    "\n",
    "ds = tfds.load(\"mtnt/en-fr\",split=\"train\")\n",
    "ds = ds.take(2)\n",
    "ds = ds.map(lambda x: {'src': tokenize_source(tokenizer, x['src']),\n",
    "                       'dst': tokenize_destination(tokenizer, x['dst'])})\n",
    "ds = ds.as_numpy_iterator()\n",
    "for idx, example in enumerate(ds):\n",
    "  print(f'Example {idx}:')\n",
    "  for key, val in example.items():\n",
    "    print(f'{key}: {val}')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-x0aTugEcoY"
   },
   "source": [
    "### Data loader\n",
    "\n",
    "We can now wrap everything a build our data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "XwFFs2mDEcoY"
   },
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingInput:\n",
    "  # Input tokens given to the model\n",
    "  input_tokens: jax.Array\n",
    "\n",
    "  # A mask that determines which tokens contribute to the target loss\n",
    "  # calculation.\n",
    "  target_mask: jax.Array\n",
    "\n",
    "class DatasetSplit(enum.Enum):\n",
    "  TRAIN = 'train'\n",
    "  VALIDATION = 'valid'\n",
    "\n",
    "\n",
    "class MTNTDatasetBuilder:\n",
    "  \"\"\"Data loader for the MTNT dataset.\"\"\"\n",
    "\n",
    "  N_ITEMS = {DatasetSplit.TRAIN: 35_692,\n",
    "             DatasetSplit.VALIDATION: 811}\n",
    "\n",
    "  BUFFER_SIZE_SHUFFLE = 10_000\n",
    "  TRANSLATION_PREFIX = 'Translate this into French:\\n'\n",
    "  TRANSLATION_SUFFIX = '\\n'\n",
    "\n",
    "  def __init__(self,\n",
    "               tokenizer : GemmaTokenizer,\n",
    "               max_seq_len: int):\n",
    "    \"\"\"Constructor.\n",
    "\n",
    "    Args:\n",
    "      tokenizer: Gemma tokenizer to use.\n",
    "      max_seq_len: size of each sequence in a given batch.\n",
    "    \"\"\"\n",
    "    self._tokenizer = tokenizer\n",
    "    self._base_data = {\n",
    "        DatasetSplit.TRAIN: tfds.load(\"mtnt/en-fr\",split=\"train\"),\n",
    "        DatasetSplit.VALIDATION: tfds.load(\"mtnt/en-fr\",split=\"valid\"),\n",
    "    }\n",
    "    self._max_seq_len = max_seq_len\n",
    "\n",
    "  def _tokenize_source(self, example: tf.Tensor):\n",
    "    \"\"\"Tokenization function for the source.\"\"\"\n",
    "    # We add <BOS> as these tokens are the start of our sequence.\n",
    "    return self._tokenizer.tokenize_tf_op(example,\n",
    "                                          prefix=self.TRANSLATION_PREFIX,\n",
    "                                          suffix=self.TRANSLATION_SUFFIX,\n",
    "                                          add_bos=True,\n",
    "                                          add_eos=False)\n",
    "\n",
    "  def _tokenize_destination(self, example: tf.Tensor):\n",
    "    \"\"\"Tokenization function for the French translation.\"\"\"\n",
    "    # We do not add <BOS> as these tokens get appended to the source tokens.\n",
    "    return self._tokenizer.tokenize_tf_op(example,\n",
    "                                          add_bos=False,\n",
    "                                          add_eos=True)\n",
    "\n",
    "  def _pad_up_to_max_len(self,\n",
    "                         input_tensor: tf.Tensor,\n",
    "                         pad_value: int | bool,\n",
    "                         ) -> tf.Tensor:\n",
    "    \"\"\"Pad the given tensor up to sequence length of a batch.\"\"\"\n",
    "    seq_len = tf.shape(input_tensor)[0]\n",
    "    to_pad = tf.maximum(self._max_seq_len - seq_len, 0)\n",
    "    return tf.pad(input_tensor,\n",
    "                  [[0, to_pad]],\n",
    "                  mode='CONSTANT',\n",
    "                  constant_values=pad_value,\n",
    "                  )\n",
    "\n",
    "  def _to_training_input(self,\n",
    "                         src_tokens: jax.Array,\n",
    "                         dst_tokens: jax.Array,\n",
    "                         ) -> TrainingInput:\n",
    "    \"\"\"Build a training input from a tuple of source and destination tokens.\"\"\"\n",
    "\n",
    "    # The input sequence fed to the model is simply the concatenation of the\n",
    "    # source and the destination.\n",
    "    tokens = tf.concat([src_tokens, dst_tokens], axis=0)\n",
    "\n",
    "    # We want to prevent the model from updating based on the source (input)\n",
    "    # tokens. To achieve this, we add a target mask to each input.\n",
    "    q_mask = tf.zeros_like(src_tokens, dtype=tf.bool)\n",
    "    a_mask = tf.ones_like(dst_tokens, dtype=tf.bool)\n",
    "    mask = tf.concat([q_mask, a_mask], axis=0)\n",
    "\n",
    "    # If the output tokens sequence is smaller than the target sequence size,\n",
    "    # then we pad it with pad tokens.\n",
    "    tokens = self._pad_up_to_max_len(tokens, self._tokenizer.pad_id)\n",
    "\n",
    "    # We don't want to perform the backward on the pad tokens.\n",
    "    mask = self._pad_up_to_max_len(mask, False)\n",
    "\n",
    "    return TrainingInput(input_tokens=tokens, target_mask=mask)\n",
    "\n",
    "\n",
    "  def get_train_dataset(self, batch_size: int, num_epochs: int):\n",
    "    \"\"\"Build the training dataset.\"\"\"\n",
    "\n",
    "    # Tokenize each sample\n",
    "    ds = self._base_data[DatasetSplit.TRAIN].map(lambda x : (self._tokenize_source(x['src']),\n",
    "                                                             self._tokenize_destination(x['dst'])))\n",
    "\n",
    "    # Convert them to training inputs\n",
    "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
    "\n",
    "    # Remove the samples which are too long\n",
    "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    ds = ds.shuffle(buffer_size=self.BUFFER_SIZE_SHUFFLE)\n",
    "\n",
    "    # Repeat if necessary\n",
    "    ds = ds.repeat(num_epochs)\n",
    "\n",
    "    # Build batches\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds\n",
    "\n",
    "  def get_validation_dataset(self, batch_size: int):\n",
    "    \"\"\"Build the validation dataset.\"\"\"\n",
    "\n",
    "    # Same as the training dataset, but no shuffling and no repetition\n",
    "    ds = self._base_data[DatasetSplit.VALIDATION].map(lambda x : (self._tokenize_source(x['src']),\n",
    "                                                                  self._tokenize_destination(x['dst'])))\n",
    "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
    "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Sq9uC15EcoZ"
   },
   "source": [
    "Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "bYeduOaNEcoZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mapping types may not work well with tf.nest. Prefer using MutableMapping for <class '__main__.TrainingInput'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "input_tokens: [[     2  49688    736   1280   6987 235292    108   1949    729  21939\n",
      "  235265    108   6870  14834  44019 235265      1      0      0      0]\n",
      " [     2  49688    736   1280   6987 235292    108    668   1941  85796\n",
      "   33727 235265    108 155869   8425 129848    519   5775      1      0]\n",
      " [     2  49688    736   1280   6987 235292    108    651  10391    576\n",
      "    5183 235265    108    651  10391    576   5183 235265      1      0]]\n",
      "target_mask: [[False False False False False False False False False False False False\n",
      "   True  True  True  True  True False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False  True  True  True  True  True  True False]\n",
      " [False False False False False False False False False False False False\n",
      "  False  True  True  True  True  True  True False]]\n",
      "\n",
      "Example 1:\n",
      "input_tokens: [[     2  49688    736   1280   6987 235292    108  12156 235341    108\n",
      "  180345  60167   1241      1      0      0      0      0      0      0]\n",
      " [     2  49688    736   1280   6987 235292    108   5739   5793 235336\n",
      "     108   2058  38263   1654      1      0      0      0      0      0]\n",
      " [     2  49688    736   1280   6987 235292    108  64555    952  73965\n",
      "  235265    108 138447  15624    581  33547  55428 155360 235265      1]]\n",
      "target_mask: [[False False False False False False False False False False  True  True\n",
      "   True  True False False False False False False]\n",
      " [False False False False False False False False False False False  True\n",
      "   True  True  True False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "   True  True  True  True  True  True  True  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GemmaTokenizer(vocab)\n",
    "dataset_builder = MTNTDatasetBuilder(tokenizer, max_seq_len=20)\n",
    "ds = dataset_builder.get_train_dataset(3, 1)\n",
    "ds = ds.take(2)\n",
    "ds = ds.as_numpy_iterator()\n",
    "for idx, example in enumerate(ds):\n",
    "  print(f'Example {idx}:')\n",
    "  for key, val in example.items():\n",
    "    print(f'{key}: {val}')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First let's load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "VDlfziQVEcoZ"
   },
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "\n",
    "# TODO: change once the downloading url is known\n",
    "params = params_lib.load_and_format_params(ckpt_path)\n",
    "\n",
    "# We use the `transformer_lib.TransformerConfig.from_params` function to\n",
    "# automatically load the correct configuration from a checkpoint. Note that the\n",
    "# vocabulary size is smaller than the number of input embeddings due to unused\n",
    "# tokens in this release.\n",
    "config_2b = transformer_lib.TransformerConfig.from_params(\n",
    "    params,\n",
    "    cache_size=30  # Number of time steps in the transformer's cache\n",
    ")\n",
    "model_2b = transformer_lib.Transformer(config=config_2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGbfx6XVEcoZ"
   },
   "source": [
    "Can our model translate French ? Well let's try it out !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "form",
    "id": "jWr6Sea_EcoZ"
   },
   "outputs": [],
   "source": [
    "sampler_old = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=params['transformer'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "id": "S6937NTjEcoZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, my name is Morgane.\\n\\nMorgane']\n"
     ]
    }
   ],
   "source": [
    "print(sampler_old(\n",
    "    [\"Translate this into French:\\nHello, my name is Morgane.\\n\"],\n",
    "    # number of steps performed when generating\n",
    "    total_generation_steps=30,\n",
    "  ).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z0CXW4REcoZ"
   },
   "source": [
    "As expected, it didn't work. Let's see if we can get better results by fine-tuning.\n",
    "\n",
    "Before moving further, don't forget to clear the memory if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "LbJa4S5WEcoZ"
   },
   "outputs": [],
   "source": [
    "del sampler_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxf6gVGCEcoZ"
   },
   "source": [
    "### Model forward and loss function\n",
    "\n",
    "Gemma `Transformer` class inherits from [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html). It offers two essential methods:\n",
    "\n",
    "- `init`: Initializes the model's parameters.\n",
    "\n",
    "- `apply`: Executes the model's `__call__` function using a given set of parameters.\n",
    "\n",
    "Since are working with pre-trained weights, we won't use the `init` function.\n",
    "\n",
    "We define a `forward_and_loss_fn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        model: transformer_lib.Transformer,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = model.apply(\n",
    "        params,\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[0, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[0, 1:]\n",
    "  target_mask = input_mask[0, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[...,None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ"
   },
   "outputs": [],
   "source": [
    "def train_step(model: transformer_lib.Transformer,\n",
    "               params,\n",
    "               optimizer: optax.GradientTransformation,\n",
    "               opt_state: optax.OptState,\n",
    "               pad_id: int,\n",
    "               example: TrainingInput):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    optimizer: optax optimizer to use.\n",
    "    opt_state: input optimizer's state.\n",
    "    pad_id: id of the pad token.\n",
    "    example: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
    "\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
    "                                                             model=model,\n",
    "                                                             input_tokens=example.input_tokens,\n",
    "                                                             input_mask=example.target_mask,\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "  # Update the parameters\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return train_loss, params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "yU4oR92YEcoa"
   },
   "outputs": [],
   "source": [
    "def validation_step(model: transformer_lib.Transformer,\n",
    "                    params,\n",
    "                    pad_id: int,\n",
    "                    example: TrainingInput,\n",
    "                    ):\n",
    "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
    "  val_loss = forward_and_loss_fn(params,\n",
    "                                 model=model,\n",
    "                                 input_tokens=example.input_tokens,\n",
    "                                 input_mask=example.target_mask,\n",
    "                                 positions=positions,\n",
    "                                 attention_mask=attention_mask)\n",
    "  return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g6LFWJbEcoa"
   },
   "source": [
    "And now the training loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "xT4bAqNLEcoa"
   },
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: transformer_lib.Transformer,\n",
    "    params,\n",
    "    dataset_builder: MTNTDatasetBuilder,\n",
    "    training_cfg: TrainingConfig):\n",
    "\n",
    "\n",
    "  # We jit the train step, making the whole loop much more efficient\n",
    "  compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
    "\n",
    "  # We do the same with the validation step\n",
    "  compiled_validation_step = jax.jit(validation_step, static_argnames=['model'])\n",
    "\n",
    "  # To save memory, we use a SGD optimizer instead of the usual Adam. Note that\n",
    "  # for this specific example SGD is more than enough.\n",
    "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  # Build the training dataset\n",
    "  train_ds = dataset_builder.get_train_dataset(batch_size=training_cfg.batch_size,\n",
    "                                               num_epochs=training_cfg.num_epochs)\n",
    "  train_ds = train_ds.as_numpy_iterator()\n",
    "\n",
    "  # Build the validation dataset, with a limited number of samples for this demo\n",
    "  validation_ds = dataset_builder.get_validation_dataset(batch_size=training_cfg.batch_size)\n",
    "  validation_ds = validation_ds.take(50)\n",
    "\n",
    "  n_steps = 0\n",
    "  avg_loss=0\n",
    "\n",
    "  # A first round of validation loss\n",
    "  n_steps_eval = 0\n",
    "  eval_loss = 0\n",
    "  val_iterator = validation_ds.as_numpy_iterator()\n",
    "  for val_example in val_iterator:\n",
    "    eval_loss += compiled_validation_step(model,\n",
    "                                          params,\n",
    "                                          dataset_builder._tokenizer.pad_id,\n",
    "                                          val_example)\n",
    "    n_steps_eval += 1\n",
    "  print(f\"Start, validation loss: {eval_loss/n_steps_eval}\")\n",
    "\n",
    "  for train_example in train_ds:\n",
    "    train_loss, params, opt_state = compiled_train_step(model=model,\n",
    "                                                        params=params,\n",
    "                                                        optimizer=optimizer,\n",
    "                                                        opt_state=opt_state,\n",
    "                                                        pad_id=dataset_builder._tokenizer.pad_id,\n",
    "                                                        example=train_example)\n",
    "    n_steps += 1\n",
    "    avg_loss += train_loss\n",
    "    if n_steps % training_cfg.eval_every_n == 0:\n",
    "      eval_loss = 0\n",
    "\n",
    "      n_steps_eval = 0\n",
    "      val_iterator = validation_ds.as_numpy_iterator()\n",
    "      for val_example in val_iterator:\n",
    "        eval_loss += compiled_validation_step(model,\n",
    "                                              params,\n",
    "                                              dataset_builder._tokenizer.pad_id,\n",
    "                                              val_example)\n",
    "        n_steps_eval +=1\n",
    "      avg_loss /= training_cfg.eval_every_n\n",
    "      eval_loss /= n_steps_eval\n",
    "      print(f\"STEP {n_steps} training loss: {avg_loss} - eval loss: {eval_loss}\")\n",
    "      avg_loss=0\n",
    "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
    "      break\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muwkf_ZgEcoa"
   },
   "source": [
    "We can fine-tune our model on a limited number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "7SL2VAmVEcoa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:19:46.228531: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start, validation loss: 3.3604793548583984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:21:17.342855: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 20 training loss: 2.1338467597961426 - eval loss: 2.3951120376586914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:21:20.214729: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 40 training loss: 2.436213493347168 - eval loss: 1.9691681861877441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:21:23.073503: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 60 training loss: 2.436979293823242 - eval loss: 1.8668928146362305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:21:25.954034: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 80 training loss: 2.340780019760132 - eval loss: 1.910522699356079\n",
      "STEP 100 training loss: 2.093810796737671 - eval loss: 2.0652272701263428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 19:21:28.821949: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Small seq size so that everything fits in memory\n",
    "SEQ_SIZE = 25\n",
    "tokenizer = GemmaTokenizer(vocab)\n",
    "dataset_builder= MTNTDatasetBuilder(tokenizer, SEQ_SIZE)\n",
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=100)\n",
    "\n",
    "params = train_loop(model=model_2b,\n",
    "                    params={'params': params['transformer']},\n",
    "                    dataset_builder=dataset_builder,\n",
    "                    training_cfg=training_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abChlybFEcod"
   },
   "source": [
    "Both the training loss and the validation's are going down. But is it working ? Let's try again with our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dQ1oCF10Ecod"
   },
   "outputs": [],
   "source": [
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=params['params'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIwhAvMsEcod"
   },
   "source": [
    "To ensure our input matches the training format, remember to use the prefix 'Translate this into French:\\n'  and a newline character at the end. This signals the model to begin translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "S5F3fk22Ecod"
   },
   "outputs": [],
   "source": [
    "sampler(\n",
    "    [\"Translate this into French:\\nHello, my name is Morgane.\\n\"],\n",
    "    total_generation_steps=30,\n",
    "    ).text\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
