{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiBSu3YkEcoX"
   },
   "source": [
    "Copyright 2024 DeepMind Technologies Limited.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5OeTiryEcoX"
   },
   "source": [
    "# Fine-tuning the 2B Gemma model with flax\n",
    "\n",
    "In this tutorial you will learn how to fine-tune the 2B Gemma model for a simple translation task. To run this colab, you will need to use a TPU v4 runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5m81VQOqEcoX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade kagglehub -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpSw-_4EEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Installation\n",
    "!pip install git+https://github.com/google-deepmind/gemma.git -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLafhtv3Rg5F"
   },
   "source": [
    "## Downloading the checkpoint\n",
    "\n",
    "\"To use Gemma's checkpoints, you'll need a Kaggle account and API key. Here's how to get them:\n",
    "\n",
    "1. Visit https://www.kaggle.com/ and create an account.\n",
    "2. Go to your account settings, then the 'API' section.\n",
    "3. Click 'Create new token' to download your key.\n",
    "\n",
    "Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow-cpu -q\n",
    "!pip install tensorflow_datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8q5seOhcUBhx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your Kaggle credentials as environment variables\n",
    "os.environ['KAGGLE_USERNAME'] = 'felarof99'\n",
    "os.environ['KAGGLE_KEY'] = '1bf21523d3337386eac3aa5d61d62b7c'\n",
    "\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCZSmEVDVv6O"
   },
   "source": [
    "If everything went well, you should see:\n",
    "```\n",
    "Kaggle credentials set.\n",
    "Kaggle credentials successfully validated.\n",
    "```\n",
    "\n",
    "Now select and download the checkpoint you want to try. On a single host, only the 2b model can fit in memory for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PEefz8wEcoY"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "VARIANT = '2b-it' # @param ['2b', '2b-it'] {type:\"string\"}\n",
    "weights_dir = kagglehub.model_download(f'google/gemma/Flax/{VARIANT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join(weights_dir, VARIANT)\n",
    "vocab_path = os.path.join(weights_dir, 'tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# We will use tensorflow to handle the dataset\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejQhgtjbEcoY"
   },
   "source": [
    "## Step 1: prepare the dataset\n",
    "\n",
    "### The MTNT dataset\n",
    "\n",
    "In this tutorial, we will use the MTNT dataset, from the paper [MTNT: A Testbed for Machine Translation of Noisy Text](https://arxiv.org/abs/1809.00388). This dataset is directly available in the [TensorFlow dataset catalog](https://www.tensorflow.org/datasets/catalog/mtnt).\n",
    "\n",
    "More precisely we will focus on the English to French translation.\n",
    "\n",
    "But let's have a look at the data themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab2MSf-qEcoY"
   },
   "source": [
    "Let's customize `SentencePieceProcessor` for our English-to-French translation task. Since we're fine-tuning the English-only Gemma 2B model, we need a few adjustments:\n",
    "\n",
    "- **Input Prefix**: Adding a common prefix to each input signals the translation task. For example we could go with a prompt like `Translate this into French: [INPUT_SENTENCE]`.\n",
    "\n",
    "- **Translation Start suffix**: We add a suffix at the end of each prompt tells the model exactly when to begin the translation process. A new line should do the job.\n",
    "\n",
    "- **LM Tokens**: Gemma models expect a *beginning of sequence* token at the beginning of each sequence. Similarly, we need to add an *end of sequence* token at the end of each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"google/gemma-2-2b-it\"\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_name, \n",
    "    token=hugging_face_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    token=hugging_face_token\n",
    ")\n",
    "\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small seq size so that everything fits in memory\n",
    "SEQ_SIZE = 25\n",
    "max_length = SEQ_SIZE\n",
    "\n",
    "# Define Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: {}\n",
    "\n",
    "### Input: {}\n",
    "\n",
    "### Response: {}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Define formatting function.\n",
    "def _format_prompts(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Tokenize the dataset.\n",
    "def _tokenize(examples):\n",
    "    # Tokenized is list within list. Compute labels for causalLM by shifting input_id; \n",
    "    # consequently truncate input_id to penultimate position.\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512+1 if not max_length else max_length+1)\n",
    "    labels = tokenized['input_ids'].copy()\n",
    "    tokenized['labels'] = [label[1:] for label in labels]\n",
    "    tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "    new_tokenized = {}\n",
    "    new_tokenized['input_tokens'] = tokenized['input_ids']\n",
    "    new_tokenized['target_mask'] = tokenized['attention_mask']\n",
    "    return new_tokenized\n",
    "\n",
    "# Load and preprocess the dataset.\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.select(range(32)) # ***DEBUG***\n",
    "dataset = dataset.map(_format_prompts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.train_test_split(test_size=0.15)\n",
    "ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "def _custom_collate_fn(batch):\n",
    "    \"Applies default_collate_fn from transformers and converts to JAX NumPy arrays.\"\"\"\n",
    "    batch = default_data_collator(batch)\n",
    "    # batch = [{k: v.numpy() for k, v in b.items()} for b in batch]\n",
    "    # batch = [{k: jnp.array(v) for k, v in b.items()} for b in batch]\n",
    "    # return batch\n",
    "    jax_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            jax_batch[key] = jnp.array(value.numpy())\n",
    "        else:\n",
    "            jax_batch[key] = value\n",
    "    \n",
    "    return jax_batch\n",
    "    \n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    ds['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=1 if not batch_size else batch_size,\n",
    "    collate_fn=_custom_collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    ds['test'],\n",
    "    shuffle=True,\n",
    "    batch_size=1 if not batch_size else batch_size,\n",
    "    collate_fn=_custom_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "#     input_ids, attention_mask = (\n",
    "#         batch[\"input_tokens\"],\n",
    "#         batch[\"target_mask\"],\n",
    "        \n",
    "#     )\n",
    "#     print(input_ids)\n",
    "#     print()\n",
    "#     print(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model\n",
    "\n",
    "### Getting started\n",
    "\n",
    "First let's load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "VDlfziQVEcoZ"
   },
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "\n",
    "# TODO: change once the downloading url is known\n",
    "params = params_lib.load_and_format_params(ckpt_path)\n",
    "\n",
    "# We use the `transformer_lib.TransformerConfig.from_params` function to\n",
    "# automatically load the correct configuration from a checkpoint. Note that the\n",
    "# vocabulary size is smaller than the number of input embeddings due to unused\n",
    "# tokens in this release.\n",
    "config_2b = transformer_lib.TransformerConfig.from_params(\n",
    "    params,\n",
    "    cache_size=30  # Number of time steps in the transformer's cache\n",
    ")\n",
    "model_2b = transformer_lib.Transformer(config=config_2b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGbfx6XVEcoZ"
   },
   "source": [
    "Can our model translate French ? Well let's try it out !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z0CXW4REcoZ"
   },
   "source": [
    "As expected, it didn't work. Let's see if we can get better results by fine-tuning.\n",
    "\n",
    "Before moving further, don't forget to clear the memory if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxf6gVGCEcoZ"
   },
   "source": [
    "### Model forward and loss function\n",
    "\n",
    "Gemma `Transformer` class inherits from [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/guides/flax_fundamentals/flax_basics.html). It offers two essential methods:\n",
    "\n",
    "- `init`: Initializes the model's parameters.\n",
    "\n",
    "- `apply`: Executes the model's `__call__` function using a given set of parameters.\n",
    "\n",
    "Since are working with pre-trained weights, we won't use the `init` function.\n",
    "\n",
    "We define a `forward_and_loss_fn` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "iEcV0XEEEcoZ"
   },
   "outputs": [],
   "source": [
    "def forward_and_loss_fn(params,\n",
    "                        *,\n",
    "                        model: transformer_lib.Transformer,\n",
    "                        input_tokens: jax.Array,            # Shape [B, L]\n",
    "                        input_mask: jax.Array,              # Shape [B, L]\n",
    "                        positions: jax.Array,               # Shape [B, L]\n",
    "                        attention_mask: jax.Array,          # [B, L, L]\n",
    "                        ) -> jax.Array:\n",
    "  \"\"\"Forward pass and loss function.\n",
    "\n",
    "  Args:\n",
    "    params: model's input parameters.\n",
    "    model: gemma transformer model to call.\n",
    "    input_tokens: input tokens sequence, shape [B, L].\n",
    "    input_mask: tokens to ignore when computing the loss, shape [B, L].\n",
    "    positions: relative position of each token, shape [B, L].\n",
    "    attention_mask: input attention mask, shape [B, L].\n",
    "\n",
    "  Returns:\n",
    "    Softmax cross-entropy loss for the next-token prediction task.\n",
    "  \"\"\"\n",
    "\n",
    "  # Forward pass on the input data.\n",
    "  # No attention cache is needed here.\n",
    "  logits, _ = model.apply(\n",
    "        params,\n",
    "        input_tokens,\n",
    "        positions,\n",
    "        None,              # Attention cache is None.\n",
    "        attention_mask,\n",
    "    )\n",
    "\n",
    "  # Exclude the last step as it does not appear in the targets.\n",
    "  logits = logits[0, :-1]\n",
    "\n",
    "  # Similarly, the first token cannot be predicteds.\n",
    "  target_tokens = input_tokens[0, 1:]\n",
    "  target_mask = input_mask[0, 1:]\n",
    "\n",
    "  # Convert the target labels into one-hot encoded vectors.\n",
    "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
    "\n",
    "  target_mask = target_mask[...,1:]\n",
    "  # Don't update on unwanted tokens.\n",
    "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[...,None]\n",
    "\n",
    "  # Normalisation factor.\n",
    "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
    "\n",
    "  # Return the nll loss.\n",
    "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y83DimpjEcoZ"
   },
   "source": [
    "The Gemma transformer requires an attention mask and position vector alongside each input. We can conveniently generate these using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cbWfdHf0EcoZ"
   },
   "outputs": [],
   "source": [
    "def get_attention_mask_and_positions(example: jax.Array,\n",
    "                                     pad_id : int,\n",
    "                                     )-> tuple[jax.Array, jax.Array]:\n",
    "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
    "  pad_mask = example != pad_id\n",
    "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
    "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
    "  return current_token_position, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbxYMMWLEcoZ"
   },
   "source": [
    "We can now build the train_step function which performs the backward pass and updates the model's parameters accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cPSfp7ZUEcoZ"
   },
   "outputs": [],
   "source": [
    "def train_step(model: transformer_lib.Transformer,\n",
    "               params,\n",
    "               optimizer: optax.GradientTransformation,\n",
    "               opt_state: optax.OptState,\n",
    "               pad_id: int,\n",
    "               example):\n",
    "  \"\"\"Train step.\n",
    "\n",
    "  Args:\n",
    "    model: gemma transformer model.\n",
    "    params: model's input parameters.\n",
    "    optimizer: optax optimizer to use.\n",
    "    opt_state: input optimizer's state.\n",
    "    pad_id: id of the pad token.\n",
    "    example: input batch.\n",
    "\n",
    "  Returns:\n",
    "    Training loss, updated parameters, updated optimizer state.\n",
    "  \"\"\"\n",
    "  # Build the position and attention mask vectors.\n",
    "  positions, attention_mask = get_attention_mask_and_positions(example['input_tokens'], pad_id)\n",
    "\n",
    "\n",
    "  # Forward and backward passes\n",
    "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
    "                                                             model=model,\n",
    "                                                             input_tokens=example['input_tokens'],\n",
    "                                                             input_mask=example['target_mask'],\n",
    "                                                             positions=positions,\n",
    "                                                             attention_mask=attention_mask)\n",
    "  # Update the parameters\n",
    "  updates, opt_state = optimizer.update(grads, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return train_loss, params, opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2QXp116EcoZ"
   },
   "source": [
    "Similarly, we build a `validation_step` function without backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6g6LFWJbEcoa"
   },
   "source": [
    "And now the training loop itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "xT4bAqNLEcoa"
   },
   "outputs": [],
   "source": [
    "@chex.dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "  learning_rate: float\n",
    "  num_epochs: int\n",
    "  eval_every_n: int\n",
    "  batch_size: int\n",
    "  max_steps: int | None = None\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: transformer_lib.Transformer,\n",
    "    params,\n",
    "    train_dataloader,\n",
    "    tokenizer,\n",
    "    training_cfg: TrainingConfig):\n",
    "\n",
    "\n",
    "  # We jit the train step, making the whole loop much more efficient\n",
    "  compiled_train_step = jax.jit(train_step , static_argnames=['model', 'optimizer'])\n",
    "\n",
    "\n",
    "  # To save memory, we use a SGD optimizer instead of the usual Adam. Note that\n",
    "  # for this specific example SGD is more than enough.\n",
    "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  n_steps = 0\n",
    "  avg_loss=0\n",
    "\n",
    "  for train_example in train_dataloader:\n",
    "    train_loss, params, opt_state = train_step(model=model,\n",
    "                                                        params=params,\n",
    "                                                        optimizer=optimizer,\n",
    "                                                        opt_state=opt_state,\n",
    "                                                        pad_id=tokenizer.pad_token_id,\n",
    "                                                        example=train_example)\n",
    "    n_steps += 1\n",
    "    avg_loss += train_loss\n",
    "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
    "      break\n",
    "  return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muwkf_ZgEcoa"
   },
   "source": [
    "We can fine-tune our model on a limited number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "7SL2VAmVEcoa"
   },
   "outputs": [],
   "source": [
    "# Small seq size so that everything fits in memory\n",
    "# SEQ_SIZE = 25\n",
    "# tokenizer = GemmaTokenizer(vocab)\n",
    "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
    "                              num_epochs=1,\n",
    "                              eval_every_n=20,\n",
    "                              batch_size=1,\n",
    "                              max_steps=100)\n",
    "\n",
    "params = train_loop(model=model_2b,\n",
    "                    params={'params': params['transformer']},\n",
    "                    train_dataloader=train_dataloader,\n",
    "                    tokenizer=tokenizer,\n",
    "                    training_cfg=training_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abChlybFEcod"
   },
   "source": [
    "Both the training loss and the validation's are going down. But is it working ? Let's try again with our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "dQ1oCF10Ecod"
   },
   "outputs": [],
   "source": [
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=model_2b,\n",
    "    vocab=vocab,\n",
    "    params=params['params'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIwhAvMsEcod"
   },
   "source": [
    "To ensure our input matches the training format, remember to use the prefix 'Translate this into French:\\n'  and a newline character at the end. This signals the model to begin translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "S5F3fk22Ecod"
   },
   "outputs": [],
   "source": [
    "sampler(\n",
    "    [\"Translate this into French:\\nHello, my name is Morgane.\\n\"],\n",
    "    total_generation_steps=30,\n",
    "    ).text\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
